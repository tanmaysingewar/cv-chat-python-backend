#helping functions related to read, write and publish model metrics
#utils
import requests
import os

from dotenv import load_dotenv
load_dotenv()
from openai import OpenAI

import re


def call_groq_api(prompt,model="gpt-4o"):
    """
    Calls the Novita AI API to interact with a specified language model, sending a prompt 
    and retrieving the model's response.

    Args:
        prompt (str): The text prompt to send to the model.
        model (str): The model to use for generating responses. Default is "meta-llama/llama-3.1-70b-instruct".

    Returns:
        str: The response content generated by the model.

    Raises:
        Exception: If there is an issue with the API call or response.

    Example:
        Input:
            prompt = "What is the capital of India?"
            model = "meta-llama/llama-3.1-70b-instruct"

        Output:
            "The capital of India is New Delhi."
    """

    client = OpenAI(
        # base_url="https://api.novita.ai/v3/openai",
        # Get the Novita AI API Key by referring to: https://novita.ai/docs/get-started/quickstart.html#_2-manage-api-key.
        api_key= os.getenv("OPENAI_API_KEY"),
    )

    # Selecting the default model
    # model = "meta-llama/llama-3.1-70b-instruct"
    model="gpt-4o"

    # Stream the response False
    stream = False 

    # Chat completion API call
    chat_completion_res = client.chat.completions.create(
        model=model,
        messages=[
             {
                "role": "user",
                "content": prompt
            }
        ],
        # stream=stream,
    )

    # Return the response
    return chat_completion_res.choices[0].message.content


# ---------------------------------------------------------------------------------------------------------------------------------------
# ---------------------------------------------------------------------------------------------------------------------------------------


# url = "https://api.perplexity.ai/chat/completions"

# def call_groq_api(prompt,modal="llama-3.1-sonar-large-128k-online"):
#     payload = {
#         "model": "llama-3.1-sonar-large-128k-online",
#         "messages": [
#             {
#                 "role": "user",
#                 "content": prompt
#             }
#         ]
#     }
#     headers = {
#         "accept": "application/json",
#         "content-type": "application/json",
#         "authorization": f"Bearer {os.getenv('PERPLEXITY_API_KEY')}"
#     }

#     response = requests.post(url, json=payload, headers=headers)
#     print(response.status_code)

#     if response.status_code == 200:
#         json_response = response.json()
#         # remove [1] this pattern from the response 
#         json_response = json_response.get("choices")[0].get("message").get("content")
#         return re.sub(r'\[\d+\]', '', json_response)
    
#     elif response.status_code == 429:
#         return response.status_code
#     elif response.status_code == 400:
#         return response.status_code
#     else:
#         return False



_all_ = [
    "call_groq_api"
]
